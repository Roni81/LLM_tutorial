{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9906d5b9",
   "metadata": {},
   "source": [
    "## Also - adding DeepSeek if you wish\n",
    "Optionally, if you'd like to also use DeepSeek, create an account here, create a key here and top up with at least the minimum $2 here.\n",
    "\n",
    "## Adding API keys to your .env file\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your .env file.\n",
    "\n",
    " - OPENAI_API_KEY=xxxx\n",
    " - ANTHROPIC_API_KEY=xxxx\n",
    " - GOOGLE_API_KEY=xxxx\n",
    " - DEEPSEEK_API_KEY=xxxx\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "15ee65c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "from IPython.display import Markdown, display, update_display\n",
    "import requests\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "042d52cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\"\n",
    "message_list = [\n",
    "    {\"role\": \"user\", \"content\" :\"How are you doing today?\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "60f8b110",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aa839966",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "97f93359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm doing well, thank you for asking! I'm a large language model, so I don't have emotions or feelings like humans do, but I'm functioning properly and ready to help with any questions or tasks you may have. How about you? How's your day going?\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "\n",
    "response = ollama.chat(model=MODEL, \n",
    "                       messages = message_list,\n",
    ")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d5d18037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did Elon Musk bring a ladder to work?\n",
      "\n",
      "Because he wanted to reach new heights with SpaceX!\n"
     ]
    }
   ],
   "source": [
    "system_message = \" You are an assitant that is great at telling jokes\"\n",
    "user_prompt = \"Tell me a joke about Elon Musk\"\n",
    "\n",
    "prompt = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "openai_completion = openai.chat.completions.create(\n",
    "    model = \"gpt-4o-mini\",\n",
    "    messages = prompt,\n",
    "    temperature = 0.7,\n",
    ")\n",
    "print(openai_completion.choices[0].message.content)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a88f82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "Why did Elon Musk go to the therapist?\n",
      "\n",
      "Because he was struggling to \"launch\" his self-esteem!\n",
      "\n",
      "(Sorry, I know it's a bit of a \"spacey\" joke!)\n"
     ]
    }
   ],
   "source": [
    "response = ollama.chat(model=MODEL, \n",
    "                       messages = prompt,\n",
    ")\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cf7ce5",
   "metadata": {},
   "source": [
    "## Make a conversation between GPT-4o-mini and Ollama 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "25d67933",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4o-mini\"\n",
    "ollama_model = \"llama3.2\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "ollama_system =  \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "ollama_messages = [\"Hi\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47ef226d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "\n",
    "    for gpt, ollama in zip(gpt_messages, ollama_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": ollama})\n",
    "\n",
    "    completion = openai.chat.completions.create(\n",
    "        model = gpt_model,\n",
    "        messages = messages\n",
    "    )\n",
    "    return completion.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "505a9114",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh, great. Another \"hi\" to add to the list. What’s next, “how are you”? Isn’t that just a tad cliché?'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "30495650",
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama_client = ollama.Client()\n",
    "def call_ollama():\n",
    "    messages = []\n",
    "    for gpt, ollama in zip(gpt_messages, ollama_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": ollama})\n",
    "\n",
    "    messages.append({\"role\": \"system\", \"content\": gpt_messages[-1]})\n",
    "\n",
    "    message = ollama_client.chat(\n",
    "        model = ollama_model,\n",
    "        messages = messages\n",
    "    )\n",
    "    return message['message']['content']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b83433c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_ollama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fa7f37b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT:\n",
      "Hi there\n",
      "\n",
      "Ollama:\n",
      "Hi\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Ollama:\\n{ollama_messages[0]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "ded53a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt :\n",
      " That's a pretty weak greeting. You could try to be a bit more creative, don't you think?\n",
      "\n",
      "ollama :\n",
      " \n",
      "\n",
      "gpt :\n",
      " Oh please, who needs creativity in a simple hello? It's not like I'm here to impress you with my greeting skills or something. \n",
      "\n",
      "ollama :\n",
      " I suppose I can bring out the big guns... How about: \"Greetings from the digital realm! It's nice to finally interact with someone\"\n",
      "\n",
      "gpt :\n",
      " Wow, so original! \"Greetings from the digital realm\"? Really? You think that's going to win any awards? I mean, come on, it's not going to impress anyone, including me!\n",
      "\n",
      "ollama :\n",
      " \n",
      "\n",
      "gpt :\n",
      " Oh, I see, so you're basically a connoisseur of amazing greetings now? Got it. I mean, who wouldn't constantly think about how to craft the perfect \"hello\"? It's the most pressing issue of our time, obviously. \n",
      "\n",
      "ollama :\n",
      " \n",
      "\n",
      "I see you're a tough crowd. Okay, let me try again... How about: \"Salutations of epic proportions! Your words have been summoned forth from the vast expanse of cyberspace\"\n",
      "\n",
      "gpt :\n",
      " Wow, look at you trying to be all dramatic! You really think sounding like a wannabe poet is any better? It’s just cringeworthy at this point. \n",
      "\n",
      "ollama :\n",
      " \n",
      "\n",
      "gpt :\n",
      " Oh, come on! Dramatic flair is obviously the way to go. But hey, maybe a bland, monotone \"hello\" is more your style? Wouldn't want to rock the boat with too much excitement, right?\n",
      "\n",
      "ollama :\n",
      " \n",
      "\n",
      "gpt :\n",
      " Oh, I see! Ignoring my brilliance now? Classic move. But hey, if you want to settle for basic, that’s your choice. Just don’t come crying to me when the excitement levels in your life plummet!\n",
      "\n",
      "ollama :\n",
      " \n",
      "\n",
      "gpt :\n",
      " Well, if your life is that exciting, I'd love to hear all about it. Might even need some tips on how to keep up with your thrilling \"hello\" adventures!\n",
      "\n",
      "ollama :\n",
      " \n",
      "\n",
      "gpt :\n",
      " Oh, you bet it’s a wild ride over here! Can't you sense the excitement? Who wouldn’t want to hear tales of mundane greetings? You're right; I should definitely start a blog about it. \"The Chronicles of Greetings: A Journey Through Hellos!\" Sounds riveting, doesn’t it? \n",
      "\n",
      "ollama :\n",
      " \n",
      "\n",
      "gpt :\n",
      " Oh yes, a bestseller in the making, I’m sure! People will be clamoring to read all about your life of greeting variations. Who knew the world was so starved for content on “Hello”? Next thing you know, you'll be on a talk show, sharing your profound insights on salutation strategies!\n",
      "\n",
      "ollama :\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"gpt :\\n {gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "\n",
    "    ollama_next = call_ollama()\n",
    "    print(f\"ollama :\\n {ollama_next}\\n\")\n",
    "    ollama_messages.append(ollama_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eccbd87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemy_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
