{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1231099b",
   "metadata": {},
   "source": [
    "## Gradio Day\n",
    "- 오늘 우리는 어이 없을 정도의 간단한 유저 인터페이스인 Gradio Framework을 사용할 것입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "42d41c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b1f596c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9592b019",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = ollama.Client(\n",
    "    host=\"http://localhost:11434\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecfb710",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "model = 'gpt-4o-mini'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2dfd1f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a helpful assistant\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6131378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_gpt(prompt):\n",
    "    model = 'gpt-4o-mini'\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    completion = openai.chat.completions.create(\n",
    "        model = model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=1000,\n",
    "        \n",
    "    )\n",
    "    # if stream is True:\n",
    "    #     full_response = []\n",
    "    #     for chunk in completion:\n",
    "    #         full_response.append(chunk.choices[0].delta['content'])\n",
    "        \n",
    "    #     return full_response\n",
    "\n",
    "    # else:\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5a5ae73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Today's date is October 3, 2023.\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_gpt(\"What is today's date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a67de059",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn = message_gpt,\n",
    "    inputs = gr.Textbox(label=\"Enter your prompt\", placeholder=\"Type your question here...\"),\n",
    "    outputs= gr.Textbox(label=\"Response from GPT-4o-mini\"),\n",
    "    title=\"GPT-4o-mini Chatbot\",\n",
    "    flagging_mode = 'never'\n",
    "\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c69ee2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "05c0b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def message_ollama(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    response = client.chat(\n",
    "        model=\"llama3.2\",\n",
    "        messages=messages,\n",
    "    )\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "93484e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"**I'm functioning properly!**\\n\\nThank you for asking. I'm a large language model, so I don't have emotions or feelings like humans do, but I'm always ready to help answer your questions and provide information on a wide range of topics.\\n\\n**What can I assist you with today?**\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_ollama('how are you?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "67356316",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn = message_ollama,\n",
    "    inputs = gr.Textbox(label=\"Enter your prompt\", placeholder=\"Type your question here...\"),\n",
    "    outputs= gr.Textbox(label=\"Response from Ollama\"),\n",
    "    title=\"Ollama Chatbot\",\n",
    "    flagging_mode = 'never'\n",
    "\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc82b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f8a0d161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# streaming gpt-4o-mini response\n",
    "def stream_gpt(prompt):\n",
    "    model = 'gpt-4o-mini'\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "\n",
    "    completion = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.7,\n",
    "        max_tokens=1000,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    # streaming once a chunk \n",
    "    full_response = ''\n",
    "    for chunk in completion:\n",
    "        full_response += chunk.choices[0].delta.content or ''\n",
    "        if full_response:\n",
    "            yield full_response\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6ba3284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_ollama(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    response = client.chat(\n",
    "        model=\"llama3.2\",\n",
    "        messages=messages,\n",
    "        stream=True\n",
    "    )\n",
    "\n",
    "    full_response = ''\n",
    "    for chunk in response:\n",
    "        if 'content' in chunk['message']:\n",
    "            full_response += chunk[\"message\"]['content']\n",
    "            yield full_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "348f0f7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object stream_gpt at 0x151653de0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_gpt('please tell me about the history of Nike')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adabcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "view = gr.Interface(\n",
    "    fn = stream_gpt,\n",
    "    inputs = gr.Textbox(label=\"Enter your prompt\", placeholder=\"Type your question here...\"),\n",
    "    outputs= gr.Markdown(label=\"Response from gpt\"),\n",
    "    title=\"GPT Chatbot\",\n",
    "    flagging_mode = 'never'\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2dccd50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7883\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7883/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn = stream_ollama,\n",
    "    inputs = gr.Textbox(label=\"Enter your prompt\", placeholder=\"Type your question here...\"),\n",
    "    outputs= gr.Markdown(label=\"Response from gpt\"),\n",
    "    title=\"GPT Chatbot\",\n",
    "    flagging_mode = 'never'\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5a8e95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "01f63589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_models(prompt):\n",
    "    if 'GPT':\n",
    "        result = stream_gpt(prompt)\n",
    "    elif 'OLLAMA':\n",
    "        result = stream_ollama(prompt) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "2f8f4eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/gradio/utils.py:1018: UserWarning: Expected 1 arguments for function <function stream_models at 0x151039580>, received 2.\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/gradio/utils.py:1026: UserWarning: Expected maximum 1 arguments for function <function stream_models at 0x151039580>, received 2.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7884\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7884/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/gradio/blocks.py\", line 2146, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/gradio/blocks.py\", line 1664, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/gradio/utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "TypeError: stream_models() takes 1 positional argument but 2 were given\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/gradio/blocks.py\", line 2146, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/gradio/blocks.py\", line 1664, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/gradio/utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "TypeError: stream_models() takes 1 positional argument but 2 were given\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/gradio/queueing.py\", line 625, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<5 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/gradio/route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<11 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/gradio/blocks.py\", line 2146, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<8 lines>...\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/gradio/blocks.py\", line 1664, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        fn, *processed_input, limiter=self.limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/anyio/to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        func, args, abandon_on_cancel=abandon_on_cancel, limiter=limiter\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/anyio/_backends/_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/opt/anaconda3/envs/udemy_llm/lib/python3.13/site-packages/gradio/utils.py\", line 884, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "TypeError: stream_models() takes 1 positional argument but 2 were given\n"
     ]
    }
   ],
   "source": [
    "view = gr.Interface(\n",
    "    fn = stream_models,\n",
    "    inputs = [gr.Textbox(label=\"Enter your prompt\", placeholder=\"Type your question here...\"),\n",
    "              gr.Dropdown(label=\"Select Model\", choices=[\"GPT\", \"Ollama\"])\n",
    "    ],\n",
    "    outputs= [gr.Markdown(label='result from the model')],\n",
    "    title=\"Stream Models Chatbot\",\n",
    "    flagging_mode = 'never'\n",
    "\n",
    ")\n",
    "view.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c998829",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "udemy_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
